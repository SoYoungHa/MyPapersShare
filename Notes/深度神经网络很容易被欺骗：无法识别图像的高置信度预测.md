# Deep Neural Networks Are Easily Fooled: High Confidence Predictions for Unrecognizable Images

# 深度神经网络很容易被欺骗：无法识别图像的高置信度预测  

Referred in [My Notes/adversarial attack](zotero://note/u/TT74AKPQ/?ignore=1&line=7)

## Abstract

DNN在图像识别领域取得了很大的成就，但之前的一篇论文： **Intriguing properties of neural networks** 提出了新的问题：只要添加一点小小的干扰DNN可能会错标图像的分类。

DNN在一些人类识别不出来是什么的图片上的置信度会达到99.99%。

***即深度神经网络的另一个缺陷：会将人类无法识别的图案以非常高的置信度归为某个label***

研究结果：阐明了人类的视觉和DNN之间有趣的差异，并且提出了有关DNN_CV通用性的问题。

![image](attachments/RWUR4U2T.png)

![image](attachments/2JYQDLJD.png)

## Conclusions

1.证明了DNN模型确实很容易被欺骗：把一些不能识别的图片很确定的分类为可识别的类型。

2\.  用了两种算法：**进化算法**和梯度上升，产生了两种不同性质类型的**Fooling images**

3.DNN并没有想象中的那么先进，和人眼还是不一样的，这让我们对DNN的泛化能力提出了疑问。

## 一、Introduction

![image](attachments/WY5D4VRH.png)

## 二、Methods

1.选择网络模型：使用了AlenNet作为代表，因为它广为人知。

2.用进化算法生成图像（EA）：

改进了下：可以同时进化很多类别如ImageNet的1000的类。

进化算法：根据适应度不断迭代选出好的（置信度高的）图像（MAP-Elites）

使用了两种不同的编码：

-   直接编码(direct encoding):
    
    <span style="background-color: rgb(255, 255, 255)">对于MNIST，28×28像素的每一个像素有一个灰度整数，对于ImageNet，256×256像素的每一个像素有三个整数（H，S，V）。每个像素值在\[0，255\]范围内用均匀随机噪声初始化。这些数字是独立变异的；首先通过以0.1开始（每个数字有10%的机会被选择变异）并以每1000次变异下降一半的速率来确定哪些数字是变异的。然后通过多项式变异算子对选择要变异的数字进行变异，变异强度固定为15。</span>
    
-   间接编码(indrect encoing):
    
    <span style="background-color: rgb(255, 255, 255)">它更可能生成规则图像（包含可压缩模式的图像（例如，对称和重复））。间接编码的图像往往是规则的，因为基因组中的元素可以影响图像的多个部分。具体地说，这里的间接编码是一个</span>**<span style="background-color: rgb(255, 255, 255)">合成模式生成网络（CPPN）</span>**<span style="background-color: rgb(255, 255, 255)">，它可以演化复杂的、规则的图像，重新组合自然和人造物体。</span>
    

![image](attachments/IVIP4LBV.png)

# 三、实验结果

<span style="background-color: rgb(255, 255, 255)">这里的规则与不规则是上一节所指的直接与间接的编码，直接编码没有几何性质，所以称为不规则的，CPPN会为图像提供几何的规则性。</span>

1.**<span style="background-color: rgb(255, 255, 255)">进化出不规则的图片来匹配MNIST</span>**

![image](attachments/TSMTSUFW.png)

​																			生成的“不规则”的0-9（置信度99.99%），人无法识别



2.**<span style="background-color: rgb(255, 255, 255)">进化出规则的图片来匹配MNIST</span>**

想看看CPPN是不是会比直接编码效果更好，产生更多的可辨识图片。

结果：有着图案，但还是会将无法识别的图像标记为数字（99.99%）

![image](attachments/G3I6B3MV.png)

<span style="background-color: rgb(255, 255, 255)">被分类为1的图片，会有竖直的线，被分类为2的图片，在底部会有水平的线。结果表明，EA利用了DNN所学习的特征来生成的图片。</span>



**3\. 不规则 match ImageNet**

提出一个假设：MNIST DNNs容易被欺骗是因为数据集太小了容易过拟合（MNIST只有60,000张训练图片）

测试猜想:使用ImageNet 2012数据集

哪怕20000次迭代后，也没有为很多类生成高置信度的图片，**平均置信度21.59%**。当然也有一些成功的，进化算法为45个类生成了置信度超过99%的图片

![image](attachments/BAQLUUID.png)

* * *

***<span style="background-color: rgba(255, 212, 0, 0.5)">4.CPNN match ImageNet</span>***

看看能不能出现图三那种人眼和神经网络都能识别出来的图片。

猜想：更大的数据集+更强的DNN架构=生成可识别的图像。

在五次独立的迭代后，CPPN生成了许多置信度大于99.99%的图片，但是无法被人识别。在经过5000轮迭代后，**平均置信度达到了88.11%。**

![image](attachments/UP3C59QQ.png)

<span style="background-color: rgb(255, 255, 255)">人无法识别由CPPN生成的图片，但是图片中确实包含一些类别的特征。也就是说，我们只需要去生成一个类中的需要的特征即可，而不需要将所有的特征都生成出来。</span>

![image](attachments/RW2S5MN6.png)

<span style="background-color: rgb(255, 255, 255)">一种图像可能同时对三个类别产生较高的置信度，但是我们通过进化算法，让图像向不同的方向进化，从而产生不同的图像类型。这说明，我们使用的是每个类别的不同的特征。</span>

***测试重复是否提高了DNN给出的图像的置信度，或者重复是否仅仅源于CPPN倾向于生成规则图像这一事实***：

![image](attachments/KFHRY45L.png)

摘除了一部分确实下降了尽管只是一点点。也证明了这一点。

也表明了**DNN倾向于去学习低层和中层特征，而不是学习对象的全局结构<span style="background-color: rgb(255, 255, 255)">。</span>**因为如果DNN学习的是全局结构，那么生成的图像里面应该包含重复的部分很少。

在这些生成的图片中，对猫和狗的攻击的表现比较差，作者说明了两个可能的解释：

1、数据集中的***猫和狗的图片更多***，也就意味着，它过拟合程度更低，也意味着其更难去欺骗

2、另一个可能的解释是因为ImageNet有太多猫和狗的类了，EA对每一个类很难找到最有代表性的图片（在该类置信度高，在别的类置信度低），而这对于以softmax为分类器的DNN来说是很重要的，***这个方面说明，数据集的类别越多，越难被攻击***。



**5.欺骗一个DNN的图像泛化到其他DNN**

Q：是否所有的DNN对同一个一类别都是学习相同的特征？

WAY：fool了这个DNN的图片能不能fool下一个

-   AB仅初始化不同结构相同
-   AB结构不同训练的数据集相同

结果显示，图片在DNN A 和DNN B 上的置信度都大于99.99%，因此DNN的泛化性可以被EA利用。



**6.防御：训练可以识别fool图片的网络**

重新训练模型，添加负面样例，将这些标记为**”fooling images”**类别，这就是对抗样本吧。

先在某个数据集上训练一个DNN 1，使用CPPN得到进化的图片，然后我们把这些图片添加到数据集上，标签为第n+1类。然后我们在新的数据集上训练DNN 2。



**7.MNIST DNN 训练fool images**

MNIST数据集总共60000张图片，共10个类别，平均每个类别6000张。为了每个类别图片数量相同。

 在第一次迭代中添加了6000张进化得到的图片（经过了3000次迭代）。每次新的迭代，我们都会增加1000张新得到的图片到训练集中。

但结果LeNet还是很一般



8.**ImageNet DNN 训练fool images**

在ImageNet上，DNN 2比DNN 1更难通过CPPN生成图片，且置信度下降。ImageNet的网络更容易去区分CPPN生成的图片与原始数据集，更容易通过反例的训练来提高自己的防御能力。

![image](attachments/TJRM5BXV.png)



**9.使用梯度上升(gradient ascent)产生欺骗图像**

另一种产生高置信度但大多数无法识别的图像的方法是在像素空间中使用梯度上升

<span style="background-color: rgb(255, 255, 255)">DNN的softmax的输出对于输入的梯度，根据梯度来增加一个单元的激活值，目的是去找到一个图片能够得到很高的分类置信度。</span>